{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hangman ML Hackathon - Complete Solution\n",
    "\n",
    "## ‚ö†Ô∏è **EXECUTION ORDER IS CRITICAL!** ‚ö†Ô∏è\n",
    "\n",
    "**You MUST run cells in order from top to bottom!**\n",
    "\n",
    "**Common Error: `NameError: name 'FirstOrderHMM' is not defined`**\n",
    "- **Cause**: You ran a cell that uses `FirstOrderHMM` before running the cell that defines it\n",
    "- **Fix**: Run Cell 16 (defines FirstOrderHMM class) before Cell 28 (uses FirstOrderHMM)\n",
    "\n",
    "**Quick Execution Guide:**\n",
    "1. **Cell 1**: Import libraries\n",
    "2. **Cells 2-13**: Load and preprocess data  \n",
    "3. **Cell 14**: Define `HangmanEnv` class\n",
    "4. **Cell 15**: Test environment\n",
    "5. **Cell 16**: **Define `FirstOrderHMM` class** ‚Üê MUST RUN BEFORE CELL 28!\n",
    "6. **Cells 17-23**: HMM training and validation\n",
    "7. **Cells 24-27**: Define RL agents (`QLearningAgent` and `DQNAgent`)\n",
    "8. **Cell 28+**: Training and evaluation\n",
    "\n",
    "**üöÄ Key Improvements:**\n",
    "- HMM-Greedy Evaluation Mode\n",
    "- Dynamic probability calculation per word\n",
    "- DQN support for better learning\n",
    "- Heavy HMM weighting (20x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hangman ML Hackathon - Complete Solution\n",
    "\n",
    "This notebook implements a complete Hangman solver using:\n",
    "1. **Hidden Markov Model (HMM)** - Language model for letter probability estimation\n",
    "2. **Reinforcement Learning (RL)** - Q-Learning agent for optimal guessing strategy\n",
    "3. **Hybrid System** - Combines HMM + RL for intelligent gameplay\n",
    "\n",
    "## Implementation Features\n",
    "\n",
    "‚úÖ **Proper Training Workflow:**\n",
    "- Stage 1: HMM Training (with validation)\n",
    "- Stage 2: RL Baseline Setup\n",
    "- Stage 3: Hybrid HMM + RL Training\n",
    "- Stage 4: Final Evaluation\n",
    "\n",
    "‚úÖ **Overfitting/Underfitting Prevention:**\n",
    "- HMM: Perplexity validation, additive smoothing\n",
    "- RL: Validation monitoring, noise injection, word shuffling\n",
    "- Comprehensive diagnostics\n",
    "\n",
    "‚úÖ **Online Learning:**\n",
    "- HMM probabilities recalculated after each guess\n",
    "- Q-values updated immediately after each action\n",
    "- Step-by-step learning (not batch learning)\n",
    "\n",
    "‚úÖ **Data Quality:**\n",
    "- Comprehensive preprocessing (case, typos, duplicates)\n",
    "- Word buckets by length\n",
    "- Train/validation/test split\n",
    "\n",
    "**Run cells sequentially from top to bottom!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found corpus: /Users/vivannaik/Desktop/ml hackathon /Data/corpus.txt\n",
      "Found test: /Users/vivannaik/Desktop/ml hackathon /Data/test.txt\n",
      "Loading raw data files...\n",
      "Raw corpus: 50000 lines\n",
      "Raw test: 2000 lines\n",
      "\n",
      "Sample raw corpus words (first 10): ['suburbanize', 'asmack', 'hypotypic', 'promoderationist', 'consonantly', 'philatelically', 'cacomelia', 'thicklips', 'luciferase', 'cinematography']\n",
      "Sample raw test words (first 10): ['marmar', 'janet', 'dentistical', 'troveless', 'unnotify', 'gastrostenosis', 'preaffiliation', 'obpyriform', 'veratrinize', 'protection']\n"
     ]
    }
   ],
   "source": [
    "# Load data files\n",
    "import os\n",
    "\n",
    "# Get the directory containing this notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else '.'))\n",
    "# Go up one level to project root\n",
    "project_root = os.path.dirname(notebook_dir) if os.path.basename(notebook_dir) == 'notebooks' else notebook_dir\n",
    "\n",
    "# Try multiple possible paths\n",
    "possible_corpus_paths = [\n",
    "    os.path.join(project_root, 'Data', 'corpus.txt'),\n",
    "    os.path.join('..', 'Data', 'corpus.txt'),\n",
    "    'Data/corpus.txt',\n",
    "    '../Data/corpus.txt',\n",
    "    os.path.join(os.getcwd(), 'Data', 'corpus.txt')\n",
    "]\n",
    "\n",
    "possible_test_paths = [\n",
    "    os.path.join(project_root, 'Data', 'test.txt'),\n",
    "    os.path.join('..', 'Data', 'test.txt'),\n",
    "    'Data/test.txt',\n",
    "    '../Data/test.txt',\n",
    "    os.path.join(os.getcwd(), 'Data', 'test.txt')\n",
    "]\n",
    "\n",
    "corpus_path = None\n",
    "test_path = None\n",
    "\n",
    "for path in possible_corpus_paths:\n",
    "    if os.path.exists(path):\n",
    "        corpus_path = path\n",
    "        break\n",
    "\n",
    "for path in possible_test_paths:\n",
    "    if os.path.exists(path):\n",
    "        test_path = path\n",
    "        break\n",
    "\n",
    "if corpus_path is None or test_path is None:\n",
    "    print(\"ERROR: Could not find data files!\")\n",
    "    print(f\"Tried corpus paths: {possible_corpus_paths}\")\n",
    "    print(f\"Tried test paths: {possible_test_paths}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Files in current dir: {os.listdir('.')}\")\n",
    "    raise FileNotFoundError(\"Data files not found! Please check the paths.\")\n",
    "\n",
    "print(f\"Found corpus: {corpus_path}\")\n",
    "print(f\"Found test: {test_path}\")\n",
    "\n",
    "# Load data\n",
    "print(\"Loading raw data files...\")\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus_raw = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Raw corpus: {len(corpus_raw)} lines\")\n",
    "\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_words_raw = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Raw test: {len(test_words_raw)} lines\")\n",
    "print(f\"\\nSample raw corpus words (first 10): {corpus_raw[:10]}\")\n",
    "print(f\"Sample raw test words (first 10): {test_words_raw[:10]}\")\n",
    "\n",
    "if len(corpus_raw) == 0:\n",
    "    raise ValueError(\"corpus_raw is empty! Check the data file.\")\n",
    "if len(test_words_raw) == 0:\n",
    "    raise ValueError(\"test_words_raw is empty! Check the data file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Cleaning and Preprocessing\n",
    "\n",
    "Comprehensive preprocessing pipeline:\n",
    "1. Case normalization (upper/lower case handling)\n",
    "2. Remove non-alphabetic characters\n",
    "3. Handle mispellings/typos (optional spell checking)\n",
    "4. Remove duplicates\n",
    "5. Filter by length and quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 136)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:136\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef preprocess_word_list(words, min_length=3, max_length=15):\u001b[39m\n                                                                 ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def normalize_case(word, case_mode='lower'):\n",
    "    \"\"\"\n",
    "    Normalize word case.\n",
    "    \n",
    "    Args:\n",
    "        word: Input word\n",
    "        case_mode: 'lower', 'upper', or 'preserve'\n",
    "    \n",
    "    Returns:\n",
    "        Normalized word\n",
    "    \"\"\"\n",
    "    if case_mode == 'lower':\n",
    "        return word.lower()\n",
    "    elif case_mode == 'upper':\n",
    "        return word.upper()\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def remove_non_alphabetic(word):\n",
    "    \"\"\"\n",
    "    Remove all non-alphabetic characters.\n",
    "    \n",
    "    Args:\n",
    "        word: Input word\n",
    "    \n",
    "    Returns:\n",
    "        Word with only alphabetic characters\n",
    "    \"\"\"\n",
    "    return ''.join([c for c in word if c.isalpha()])\n",
    "\n",
    "def remove_special_characters(word):\n",
    "    \"\"\"\n",
    "    Remove special characters but keep alphabetic and basic characters.\n",
    "    \"\"\"\n",
    "    # Keep only letters and basic characters\n",
    "    return re.sub(r'[^a-zA-Z]', '', word)\n",
    "\n",
    "def fix_common_typos(word):\n",
    "    \"\"\"\n",
    "    Fix common typos and character confusions.\n",
    "    \"\"\"\n",
    "    # Common typo fixes\n",
    "    typo_fixes = {\n",
    "        # Common character confusions\n",
    "        '0': 'o', 'O': 'o',\n",
    "        '1': 'i', 'I': 'i',\n",
    "        '3': 'e',\n",
    "        '5': 's',\n",
    "        '@': 'a',\n",
    "        # Remove numbers and special chars (handled separately)\n",
    "    }\n",
    "    \n",
    "    fixed = word\n",
    "    for typo, correct in typo_fixes.items():\n",
    "        fixed = fixed.replace(typo, correct)\n",
    "    \n",
    "    return fixed\n",
    "\n",
    "def is_valid_word(word, min_length=2, max_length=25):\n",
    "    \"\"\"\n",
    "    Check if word is valid after cleaning.\n",
    "    \n",
    "    Args:\n",
    "        word: Word to check\n",
    "        min_length: Minimum valid length\n",
    "        max_length: Maximum valid length\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if word is valid\n",
    "    \"\"\"\n",
    "    if not word:\n",
    "        return False\n",
    "    \n",
    "    if len(word) < min_length or len(word) > max_length:\n",
    "        return False\n",
    "    \n",
    "    # Check if word contains only alphabetic characters\n",
    "    if not word.isalpha():\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def remove_duplicates_keep_order(words):\n",
    "    \"\"\"\n",
    "    Remove duplicates while preserving order.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            unique_words.append(word)\n",
    "    return unique_words\n",
    "\n",
    "def clean_and_preprocess_word(word, \n",
    "                             normalize_case_mode='lower',\n",
    "                             remove_non_alpha=True,\n",
    "                             fix_typos=True,\n",
    "                             min_length=2,\n",
    "                             max_length=25):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for a single word.\n",
    "    \n",
    "    Args:\n",
    "        word: Raw word\n",
    "        normalize_case_mode: 'lower', 'upper', or 'preserve'\n",
    "        remove_non_alpha: Remove non-alphabetic characters\n",
    "        fix_typos: Attempt to fix common typos\n",
    "        min_length: Minimum word length\n",
    "        max_length: Maximum word length\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned word or None if invalid\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize case\n",
    "    cleaned = normalize_case(word, normalize_case_mode)\n",
    "    \n",
    "    # Step 2: Fix common typos (do before removing non-alpha to catch digit confusions)\n",
    "    if fix_typos:\n",
    "        cleaned = fix_common_typos(cleaned)\n",
    "    \n",
    "    # Step 3: Remove non-alphabetic characters\n",
    "    if remove_non_alpha:\n",
    "        cleaned = remove_non_alphabetic(cleaned)\n",
    "    \n",
    "    # Step 4: Validate\n",
    "    if not is_valid_word(cleaned, min_length, max_length):\n",
    "        return None\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "  def preprocess_word_list(words, min_length=3, max_length=15):\n",
    "    # Convert to lowercase and remove duplicates\n",
    "    words = list({word.lower() for word in words})\n",
    "    # Filter words by length and letters only\n",
    "    return sorted([word for word in words \n",
    "                  if (min_length <= len(word) <= max_length and \n",
    "                      word.isalpha())])\n",
    "    # Convert to lowercase and remove duplicates\n",
    "    words = list({word.lower() for word in words})\n",
    "    \n",
    "    # Filter words based on length and character set (only letters)\n",
    "    filtered_words = [\n",
    "        word for word in words \n",
    "        if (min_length <= len(word) <= max_length and \n",
    "            word.isalpha())\n",
    "    ]\n",
    "    \n",
    "    return sorted(filtered_words)\n",
    "    return sorted(filtered_words)\n",
    "    for word in words:\n",
    "        original = word\n",
    "        \n",
    "        # Clean word\n",
    "        cleaned = clean_and_preprocess_word(\n",
    "            word,\n",
    "            normalize_case_mode=normalize_case_mode,\n",
    "            remove_non_alpha=remove_non_alpha,\n",
    "            fix_typos=fix_typos,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        if cleaned is not None:\n",
    "            cleaned_words.append(cleaned)\n",
    "        else:\n",
    "            stats['removed_words'].append(original)\n",
    "    \n",
    "    stats['after_length_filter'] = len(cleaned_words)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    if remove_duplicates:\n",
    "        before_dup_removal = len(cleaned_words)\n",
    "        cleaned_words = remove_duplicates_keep_order(cleaned_words)\n",
    "        stats['after_duplicate_removal'] = len(cleaned_words)\n",
    "        stats['duplicates_removed'] = before_dup_removal - len(cleaned_words)\n",
    "    \n",
    "    stats['final_count'] = len(cleaned_words)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Preprocessing Statistics:\")\n",
    "        print(f\"  Original words: {stats['original_count']}\")\n",
    "        print(f\"  After cleaning: {stats['final_count']}\")\n",
    "        print(f\"  Removed: {stats['original_count'] - stats['final_count']}\")\n",
    "        if remove_duplicates:\n",
    "            print(f\"  Duplicates removed: {stats.get('duplicates_removed', 0)}\")\n",
    "        if stats.get('original_count', 0) > 0:\n",
    "        if stats.get('original_count', 0) > 0:\n",
    "            print(f\"  Removal rate: {(stats['original_count'] - stats['final_count']) / stats['original_count'] * 100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"  Removal rate: N/A (original count was 0)\")\n",
    "    else:\n",
    "        print(f\"  Removal rate: N/A (original count was 0)\")\n",
    "    \n",
    "    return cleaned_words, stats\n",
    "\n",
    "print(\"Preprocessing functions defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess corpus\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING CORPUS\")\n",
    "print(\"=\"*60)\n",
    "corpus, corpus_stats = preprocess_word_list(\n",
    "    corpus_raw,\n",
    "    normalize_case_mode='lower',  # Convert all to lowercase\n",
    "    remove_non_alpha=True,\n",
    "    fix_typos=True,\n",
    "    min_length=3,   # Minimum 3 letters (adjust as needed)\n",
    "    max_length=20,  # Maximum 20 letters\n",
    "    remove_duplicates=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCorpus sample (first 20): {corpus[:20]}\")\n",
    "\n",
    "# Show some removed words (if any)\n",
    "if len(corpus_stats['removed_words']) > 0:\n",
    "    print(f\"\\nSample of removed words (first 10): {corpus_stats['removed_words'][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING TEST SET\")\n",
    "print(\"=\"*60)\n",
    "test_words, test_stats = preprocess_word_list(\n",
    "    test_words_raw,\n",
    "    normalize_case_mode='lower',  # Same as corpus\n",
    "    remove_non_alpha=True,\n",
    "    fix_typos=True,\n",
    "    min_length=3,   # Same as corpus\n",
    "    max_length=20,  # Same as corpus\n",
    "    remove_duplicates=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTest set sample (first 20): {test_words[:20]}\")\n",
    "\n",
    "# Check for overlap between corpus and test (data leakage check)\n",
    "corpus_set = set(corpus)\n",
    "test_set = set(test_words)\n",
    "overlap = corpus_set & test_set\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA LEAKAGE CHECK\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Words in corpus: {len(corpus_set)}\")\n",
    "print(f\"Words in test: {len(test_set)}\")\n",
    "print(f\"Overlapping words: {len(overlap)}\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {len(overlap)} words appear in both corpus and test set!\")\n",
    "    print(f\"   Sample overlap: {list(overlap)[:10]}\")\n",
    "    print(f\"   This may cause data leakage. Consider removing from test set.\")\n",
    "else:\n",
    "    print(f\"‚úÖ No overlap detected - good data separation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Quality Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality analysis\n",
    "def analyze_word_quality(words, name=\"Dataset\"):\n",
    "    \"\"\"Analyze quality metrics of word list\"\"\"\n",
    "    if not words:\n",
    "        print(f\"{name}: No words to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Length distribution\n",
    "    lengths = [len(w) for w in words]\n",
    "    \n",
    "    # Character analysis\n",
    "    all_chars = ''.join(words)\n",
    "    char_counts = Counter(all_chars)\n",
    "    \n",
    "    # Unique characters\n",
    "    unique_chars = set(all_chars)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name} QUALITY ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total words: {len(words)}\")\n",
    "    print(f\"Unique words: {len(set(words))}\")\n",
    "    print(f\"Average length: {np.mean(lengths):.2f} characters\")\n",
    "    print(f\"Min length: {min(lengths)}\")\n",
    "    print(f\"Max length: {max(lengths)}\")\n",
    "    print(f\"Most common length: {Counter(lengths).most_common(1)[0][0]} letters ({Counter(lengths).most_common(1)[0][1]} words)\")\n",
    "    print(f\"\\nUnique characters: {len(unique_chars)}\")\n",
    "    print(f\"All lowercase: {all(w.islower() for w in words)}\")\n",
    "    print(f\"All alphabetic: {all(w.isalpha() for w in words)}\")\n",
    "    \n",
    "    # Character frequency\n",
    "    print(f\"\\nTop 10 most common characters:\")\n",
    "    for char, count in char_counts.most_common(10):\n",
    "        freq = count / len(all_chars) * 100\n",
    "        print(f\"  '{char}': {count:,} times ({freq:.2f}%)\")\n",
    "    \n",
    "    # Visualize length distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(lengths, bins=range(min(lengths), max(lengths)+2), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Word Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{name} - Word Length Distribution')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze cleaned datasets\n",
    "analyze_word_quality(corpus, \"CORPUS\")\n",
    "analyze_word_quality(test_words, \"TEST SET\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split into Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split corpus into training and validation sets\n",
    "# Use 90% for training, 10% for validation\n",
    "split_idx = int(len(corpus) * 0.9)\n",
    "training_corpus = corpus[:split_idx]\n",
    "validation_corpus = corpus[split_idx:]\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Training: {len(training_corpus)} words ({len(training_corpus)/len(corpus)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(validation_corpus)} words ({len(validation_corpus)/len(corpus)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_words)} words\")\n",
    "\n",
    "# Shuffle training data for better distribution\n",
    "random.shuffle(training_corpus)\n",
    "random.shuffle(validation_corpus)\n",
    "\n",
    "print(f\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(f\"   All words are lowercase, alphabetic, and within length range [3, 20]\")\n",
    "\n",
    "\n",
    "# Guards to ensure non-empty datasets\n",
    "if len(training_corpus) == 0 and len(corpus) > 0:\n",
    "    training_corpus = corpus[: max(1, int(0.9*len(corpus)))]\n",
    "if len(validation_corpus) == 0 and len(corpus) > 0:\n",
    "    validation_corpus = corpus[-max(1, len(corpus)-len(training_corpus)):]\n",
    "if 'test_words' in globals() and len(test_words) == 0 and 'test_words_raw' in globals() and len(test_words_raw) > 0:\n",
    "    test_words = [w.strip().lower() for w in test_words_raw if w and w.strip().isalpha() and 3 <= len(w) <= 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Organize Words into Buckets by Length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 RL Agent - Hyperparameters and Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hangman Environment\n",
    "\n",
    "**IMPORTANT**: Run this cell to define the `HangmanEnv` class before using it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Agent Hyperparameters\n",
    "RL_CONFIG = {\n",
    "    'learning_rate': 0.2,        # Œ±: Further increased for faster learning\n",
    "    'discount_factor': 0.99,     # Œ≥: Higher for very long-term planning\n",
    "    'epsilon': 1.0,              # Initial exploration (100% random)\n",
    "    'epsilon_decay': 0.999,      # Slower decay = more exploration time\n",
    "    'epsilon_min': 0.02,         # Lower min (less random at end, trust HMM more)\n",
    ",\n",
    "    'q_weight': 1.0\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RL AGENT HYPERPARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Learning Rate (Œ±): {RL_CONFIG['learning_rate']}\")\n",
    "print(f\"  ‚Üí Too high: Oscillations, unstable learning\")\n",
    "print(f\"  ‚Üí Too low: Slow convergence\")\n",
    "print(f\"  ‚Üí Recommended: 0.05-0.2 for Q-learning\")\n",
    "print(f\"\\nDiscount Factor (Œ≥): {RL_CONFIG['discount_factor']}\")\n",
    "print(f\"  ‚Üí High (0.95-0.99): Focus on long-term rewards\")\n",
    "print(f\"  ‚Üí Low (0.7-0.9): Focus on immediate rewards\")\n",
    "print(f\"\\nExploration Schedule:\")\n",
    "print(f\"  ‚Üí Initial Œµ: {RL_CONFIG['epsilon']} (100% exploration)\")\n",
    "print(f\"  ‚Üí Decay: {RL_CONFIG['epsilon_decay']} per episode\")\n",
    "print(f\"  ‚Üí Final Œµ: {RL_CONFIG['epsilon_min']} (1% exploration)\")\n",
    "print(f\"  ‚Üí Episodes to 50%: {np.log(0.5) / np.log(RL_CONFIG['epsilon_decay']):.0f}\")\n",
    "print(f\"\\nüí° Tuning Tips:\")\n",
    "print(f\"  - If win rate not improving: Try higher learning rate or stronger rewards\")\n",
    "print(f\"  - If too random: Increase epsilon_decay (faster exploitation)\")\n",
    "print(f\"  - If stuck in local optimum: Increase epsilon_min (more exploration)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation mode: 'hybrid' uses Q+HMM, 'hmm' uses pure HMM\n",
    "EVAL_MODE = 'hybrid'\n",
    "print('EVAL_MODE =', EVAL_MODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM Model\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL**: Run this cell to define `FirstOrderHMM` class!  \n",
    "**You MUST run this before any cell that uses `hmm = FirstOrderHMM(...)`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize words into buckets by length\n",
    "def bucket_words_by_length(words, min_length=3, max_length=20):\n",
    "    \"\"\"\n",
    "    Organize words into buckets by length.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {length: [list of words]}\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    \n",
    "    for word in words:\n",
    "        word_len = len(word)\n",
    "        if min_length <= word_len <= max_length:\n",
    "            buckets[word_len].append(word)\n",
    "    \n",
    "    return buckets\n",
    "\n",
    "# Bucket all datasets by length\n",
    "training_buckets = bucket_words_by_length(training_corpus)\n",
    "validation_buckets = bucket_words_by_length(validation_corpus)\n",
    "test_buckets = bucket_words_by_length(test_words)\n",
    "\n",
    "print(\"Word buckets by length:\")\n",
    "print(f\"\\nTraining corpus buckets:\")\n",
    "for length in sorted(training_buckets.keys()):\n",
    "    print(f\"  Length {length:2d}: {len(training_buckets[length]):5d} words\")\n",
    "\n",
    "print(f\"\\nValidation corpus buckets:\")\n",
    "for length in sorted(validation_buckets.keys()):\n",
    "    print(f\"  Length {length:2d}: {len(validation_buckets[length]):5d} words\")\n",
    "\n",
    "print(f\"\\nTest corpus buckets:\")\n",
    "for length in sorted(test_buckets.keys()):\n",
    "    print(f\"  Length {length:2d}: {len(test_buckets[length]):5d} words\")\n",
    "\n",
    "# Visualize word length distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Training distribution\n",
    "train_lengths = sorted(training_buckets.keys())\n",
    "train_counts = [len(training_buckets[l]) for l in train_lengths]\n",
    "axes[0].bar(train_lengths, train_counts, color='blue', alpha=0.7)\n",
    "axes[0].set_xlabel('Word Length')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Training Corpus - Word Length Distribution')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Validation distribution\n",
    "val_lengths = sorted(validation_buckets.keys())\n",
    "val_counts = [len(validation_buckets[l]) for l in val_lengths]\n",
    "axes[1].bar(val_lengths, val_counts, color='orange', alpha=0.7)\n",
    "axes[1].set_xlabel('Word Length')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Validation Corpus - Word Length Distribution')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Test distribution\n",
    "test_lengths = sorted(test_buckets.keys())\n",
    "test_counts = [len(test_buckets[l]) for l in test_lengths]\n",
    "axes[2].bar(test_lengths, test_counts, color='green', alpha=0.7)\n",
    "axes[2].set_xlabel('Word Length')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Test Corpus - Word Length Distribution')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Training: {sum(len(words) for words in training_buckets.values())} words\")\n",
    "print(f\"Validation: {sum(len(words) for words in validation_buckets.values())} words\")\n",
    "print(f\"Test: {sum(len(words) for words in test_buckets.values())} words\")\n",
    "print(f\"\\nMost common length in training: {max(training_buckets.keys(), key=lambda k: len(training_buckets[k]))} letters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Letter Frequency Heuristics\n",
    "\n",
    "Common English letter frequencies as fallback when HMM is uncertain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We NO LONGER use predetermined letter frequencies\n",
    "# Instead, HMM calculates DYNAMIC probabilities for EACH word based on:\n",
    "# - Word-specific context (known letters, position)\n",
    "# - Bigram patterns in this specific word\n",
    "# - Position-specific probabilities for this word length\n",
    "# - Transition probabilities from this word's context\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DYNAMIC PROBABILITY CALCULATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úì Probabilities calculated dynamically for EACH word\")\n",
    "print(\"‚úì Uses word-specific context (length, position, known letters)\")\n",
    "print(\"‚úì Top 5 suggestions generated dynamically per word\")\n",
    "print(\"‚úì NO predetermined/fixed probabilities\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hangman Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanEnv:\n",
    "    \"\"\"Hangman game environment\"\"\"\n",
    "    \n",
    "    def __init__(self, word, max_lives=6, max_guesses=25):\n",
    "        self.word = word.lower()\n",
    "        self.max_lives = max_lives\n",
    "        self.max_guesses = max_guesses\n",
    "        self.lives = max_lives\n",
    "        self.guessed_letters = set()\n",
    "        self.masked_word = ['_' for _ in self.word]\n",
    "        self.num_guesses = 0\n",
    "        \n",
    "        # Reward parameters (tunable)\n",
    "        self.reward_correct = 0.5\n",
    "        self.reward_wrong = -1.0\n",
    "        self.reward_repeated = -0.5\n",
    "        self.reward_win = 10.0\n",
    "        self.reward_lose = -5.0\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get current state representation\"\"\"\n",
    "        masked_str = ''.join(self.masked_word)\n",
    "        return {\n",
    "            'masked_word': masked_str,\n",
    "            'guessed_letters': self.guessed_letters.copy(),\n",
    "            'lives_left': self.lives,\n",
    "            'word_length': len(self.word),\n",
    "            'num_guesses': self.num_guesses\n",
    "        }\n",
    "    \n",
    "    def guess_letter(self, letter):\n",
    "        \"\"\"\n",
    "        Guess a letter. Returns (reward, new_state, done, info)\n",
    "        \"\"\"\n",
    "        letter = letter.lower()\n",
    "        self.num_guesses += 1\n",
    "        \n",
    "        # Check repeated guess\n",
    "        if letter in self.guessed_letters:\n",
    "            reward = self.reward_repeated\n",
    "            return reward, self.get_state(), False, {'status': 'repeated'}\n",
    "        \n",
    "        self.guessed_letters.add(letter)\n",
    "        \n",
    "        # Check if correct\n",
    "        if letter in self.word:\n",
    "            # Update masked word\n",
    "            for i, char in enumerate(self.word):\n",
    "                if char == letter:\n",
    "                    self.masked_word[i] = letter\n",
    "            \n",
    "            # Check win\n",
    "            if '_' not in self.masked_word:\n",
    "                reward = self.reward_win\n",
    "                return reward, self.get_state(), True, {'status': 'won'}\n",
    "            else:\n",
    "                reward = self.reward_correct\n",
    "                return reward, self.get_state(), False, {'status': 'correct'}\n",
    "        else:\n",
    "            # Wrong guess\n",
    "            self.lives -= 1\n",
    "            \n",
    "            if self.lives == 0 or self.num_guesses >= self.max_guesses:\n",
    "                reward = self.reward_lose\n",
    "                return reward, self.get_state(), True, {'status': 'lost'}\n",
    "            else:\n",
    "                reward = self.reward_wrong\n",
    "                return reward, self.get_state(), False, {'status': 'wrong'}\n",
    "    \n",
    "    def reset(self, word=None):\n",
    "        \"\"\"Reset environment with new word\"\"\"\n",
    "        if word:\n",
    "            self.word = word.lower()\n",
    "        self.lives = self.max_lives\n",
    "        self.guessed_letters = set()\n",
    "        self.masked_word = ['_' for _ in self.word]\n",
    "        self.num_guesses = 0\n",
    "        return self.get_state()\n",
    "\n",
    "# Test environment\n",
    "print(\"Testing Hangman Environment...\")\n",
    "env = HangmanEnv(\"apple\")\n",
    "print(f\"Initial state: {env.get_state()['masked_word']}\")\n",
    "\n",
    "reward, state, done, info = env.guess_letter('a')\n",
    "print(f\"Guess 'a': {state['masked_word']}, reward={reward}, done={done}\")\n",
    "\n",
    "reward, state, done, info = env.guess_letter('p')\n",
    "print(f\"Guess 'p': {state['masked_word']}, reward={reward}, done={done}\")\n",
    "\n",
    "reward, state, done, info = env.guess_letter('x')\n",
    "print(f\"Guess 'x': {state['masked_word']}, reward={reward}, lives={state['lives_left']}, done={done}\")\n",
    "print(\"Environment working correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstOrderHMM:\n",
    "    \"\"\"\n",
    "    1st-order HMM: P(word) = P(l1, l2, ..., ln) = ‚àè P(li | li-1)\n",
    "    \n",
    "    Structure:\n",
    "    - Hidden states: letters (26 states) + start/end tokens\n",
    "    - Emissions: letters (26 observations)\n",
    "    - Transitions: A[i][j] = P(letter_j | letter_i)\n",
    "    - Emissions: B[i][k] = P(obs_k | state_i) = identity (state emits itself)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=0.01):\n",
    "        self.smoothing = smoothing  # Additive (Laplace) smoothing parameter\n",
    "        self.vocab_size = 26\n",
    "        \n",
    "        # Transition matrix: A[i][j] = P(j | i)\n",
    "        # i, j are letter indices (0=a, 1=b, ..., 25=z)\n",
    "        self.transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.transition_probs = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Start probabilities: P(first letter)\n",
    "        self.start_counts = defaultdict(int)\n",
    "        self.start_probs = defaultdict(float)\n",
    "        \n",
    "        # End probabilities: P(end | last letter)\n",
    "        self.end_counts = defaultdict(int)\n",
    "        self.end_probs = defaultdict(float)\n",
    "        \n",
    "        # Letter frequency (for fallback)\n",
    "        self.letter_freq = defaultdict(int)\n",
    "        self.total_letters = 0\n",
    "        \n",
    "        # Position-based probabilities (fallback for different word lengths)\n",
    "        self.position_letter_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "        \n",
    "        # Bigram patterns for common word structures\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        \n",
    "    def train(self, corpus, max_order=1):\n",
    "        \"\"\"Train 1st-order HMM on corpus\"\"\"\n",
    "        print(\"Training 1st-order HMM...\")\n",
    "        \n",
    "        # Count transitions and positions\n",
    "        # Also track bigrams and common patterns\n",
    "        bigram_counts = defaultdict(int)  # Track common bigrams like \"AP\", \"PP\", etc.\n",
    "        \n",
    "        for word in corpus:\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Track position-based counts (for position-dependent predictions)\n",
    "            word_len = len(word)\n",
    "            for pos, letter in enumerate(word):\n",
    "                self.position_letter_counts[word_len][pos][letter] += 1\n",
    "                self.letter_freq[letter] += 1\n",
    "                self.total_letters += 1\n",
    "            \n",
    "            # Track transitions (1st-order: letter depends on previous letter)\n",
    "            for i in range(len(word)):\n",
    "                letter = word[i]\n",
    "                \n",
    "                if i == 0:\n",
    "                    # Start probability\n",
    "                    self.start_counts[letter] += 1\n",
    "                else:\n",
    "                    # Transition: previous letter -> current letter\n",
    "                    prev_letter = word[i-1]\n",
    "                    self.transition_counts[prev_letter][letter] += 1\n",
    "                    # Track bigram\n",
    "                    bigram = prev_letter + letter\n",
    "                    bigram_counts[bigram] += 1\n",
    "                \n",
    "                # End probability: last letter -> end\n",
    "                if i == len(word) - 1:\n",
    "                    self.end_counts[letter] += 1\n",
    "        \n",
    "        # Store bigram counts for pattern matching\n",
    "        self.bigram_counts = bigram_counts\n",
    "        \n",
    "        # Compute transition probabilities with smoothing\n",
    "        print(\"Computing transition probabilities...\")\n",
    "        for prev_letter in self.transition_counts:\n",
    "            total = sum(self.transition_counts[prev_letter].values())\n",
    "            for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "                count = self.transition_counts[prev_letter].get(letter, 0)\n",
    "                # Additive smoothing: P(j|i) = (count + Œ±) / (total + Œ± * vocab_size)\n",
    "                self.transition_probs[prev_letter][letter] = (count + self.smoothing) / (total + self.smoothing * self.vocab_size)\n",
    "        \n",
    "        # Compute start probabilities\n",
    "        total_starts = sum(self.start_counts.values())\n",
    "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            count = self.start_counts.get(letter, 0)\n",
    "            self.start_probs[letter] = (count + self.smoothing) / (total_starts + self.smoothing * self.vocab_size)\n",
    "        \n",
    "        # Compute end probabilities\n",
    "        total_ends = sum(self.end_counts.values())\n",
    "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            count = self.end_counts.get(letter, 0)\n",
    "            self.end_probs[letter] = (count + self.smoothing) / (total_ends + self.smoothing * self.vocab_size)\n",
    "        \n",
    "        print(f\"Trained on {len(corpus)} words\")\n",
    "        print(f\"Total letter transitions: {sum(sum(v.values()) for v in self.transition_counts.values())}\")\n",
    "    \n",
    "    def get_letter_probability_given_prev(self, prev_letter, letter):\n",
    "        \"\"\"Get P(letter | prev_letter) using transition probabilities\"\"\"\n",
    "        if prev_letter is None:\n",
    "            # Start probability\n",
    "            return self.start_probs.get(letter, self.smoothing / self.vocab_size)\n",
    "        return self.transition_probs.get(prev_letter, {}).get(letter, self.smoothing / self.vocab_size)\n",
    "    \n",
    "    def get_letter_probability_by_position(self, word_length, position, letter):\n",
    "        \"\"\"Get P(letter | position, length) - position-based fallback\"\"\"\n",
    "        if word_length not in self.position_letter_counts:\n",
    "            return self.letter_freq.get(letter, self.smoothing) / (self.total_letters + self.smoothing * self.vocab_size)\n",
    "        \n",
    "        if position not in self.position_letter_counts[word_length]:\n",
    "            return self.letter_freq.get(letter, self.smoothing) / (self.total_letters + self.smoothing * self.vocab_size)\n",
    "        \n",
    "        counts = self.position_letter_counts[word_length][position]\n",
    "        total = sum(counts.values()) + self.smoothing * self.vocab_size\n",
    "        \n",
    "        letter_count = counts.get(letter, 0) + self.smoothing\n",
    "        return letter_count / total\n",
    "    \n",
    "    def get_letter_probability_given_next(self, letter, next_letter):\n",
    "        \"\"\"Get P(letter | next_letter) - reverse transition\"\"\"\n",
    "        if next_letter is None:\n",
    "            # End probability (reverse)\n",
    "            return self.end_probs.get(letter, self.smoothing / self.vocab_size)\n",
    "        # We need reverse transitions: P(letter | next_letter)\n",
    "        # Approximate using forward transitions: P(next_letter | letter)\n",
    "        # Then use Bayes' rule approximation\n",
    "        forward_prob = self.transition_probs.get(letter, {}).get(next_letter, self.smoothing / self.vocab_size)\n",
    "        letter_freq = self.letter_freq.get(letter, self.smoothing) / max(self.total_letters, 1)\n",
    "        next_freq = self.letter_freq.get(next_letter, self.smoothing) / max(self.total_letters, 1)\n",
    "        if next_freq > 0:\n",
    "            reverse_prob = (forward_prob * letter_freq) / next_freq\n",
    "            return min(reverse_prob, 1.0)\n",
    "        return forward_prob\n",
    "    \n",
    "    def get_probabilities_for_mask(self, masked_word, guessed_letters=set()):\n",
    "        \"\"\"\n",
    "        Get probability distribution over alphabet for given masked word.\n",
    "        IMPROVED: Uses bidirectional context (previous AND next letters).\n",
    "        \"\"\"\n",
    "        word_length = len(masked_word)\n",
    "        letter_probs = defaultdict(float)\n",
    "        \n",
    "        # Get known letters and their positions\n",
    "        known_letters = {}\n",
    "        blank_positions = []\n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char == '_':\n",
    "                blank_positions.append(i)\n",
    "            else:\n",
    "                known_letters[i] = char\n",
    "        \n",
    "        if not blank_positions:\n",
    "            return {letter: 0 for letter in 'abcdefghijklmnopqrstuvwxyz'}\n",
    "        \n",
    "        # For each blank position, calculate probabilities using BIDIRECTIONAL context\n",
    "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            if letter in guessed_letters:\n",
    "                letter_probs[letter] = 0\n",
    "                continue\n",
    "            \n",
    "            position_probs = []\n",
    "            \n",
    "            for blank_pos in blank_positions:\n",
    "                # Strategy 1: Forward context (previous letter)\n",
    "                prev_letter = None\n",
    "                for i in range(blank_pos - 1, -1, -1):\n",
    "                    if i in known_letters:\n",
    "                        prev_letter = known_letters[i]\n",
    "                        break\n",
    "                \n",
    "                forward_prob = self.get_letter_probability_given_prev(prev_letter, letter)\n",
    "                \n",
    "                # Strategy 2: Backward context (next letter) - NEW!\n",
    "                next_letter = None\n",
    "                for i in range(blank_pos + 1, len(masked_word)):\n",
    "                    if i in known_letters:\n",
    "                        next_letter = known_letters[i]\n",
    "                        break\n",
    "                \n",
    "                backward_prob = self.get_letter_probability_given_next(letter, next_letter)\n",
    "                \n",
    "                # Strategy 3: Position-based probabilities\n",
    "                pos_prob = self.get_letter_probability_by_position(word_length, blank_pos, letter)\n",
    "                \n",
    "                # Strategy 4: Bigram pattern matching (improved)\n",
    "                # Check if letter fits common patterns\n",
    "                pattern_boost = 1.0\n",
    "                \n",
    "                # Check bigram patterns\n",
    "                if prev_letter:\n",
    "                    bigram_prev = prev_letter + letter\n",
    "                    bigram_freq = self.bigram_counts.get(bigram_prev, 0)\n",
    "                    if bigram_freq > 100:  # Common bigram\n",
    "                        pattern_boost *= (1.0 + min(bigram_freq / 1000, 1.0))  # Boost up to 2x\n",
    "                \n",
    "                if next_letter:\n",
    "                    bigram_next = letter + next_letter\n",
    "                    bigram_freq = self.bigram_counts.get(bigram_next, 0)\n",
    "                    if bigram_freq > 100:  # Common bigram\n",
    "                        pattern_boost *= (1.0 + min(bigram_freq / 1000, 1.0))  # Boost up to 2x\n",
    "                \n",
    "                # Special case: Common patterns like \"_PPLE\" -> should suggest 'A'\n",
    "                if blank_pos == 0 and next_letter == 'p':\n",
    "                    # Check if \"AP\" is a common bigram\n",
    "                    if self.bigram_counts.get('ap', 0) > 50:\n",
    "                        if letter == 'a':\n",
    "                            pattern_boost *= 3.0  # Strong boost for \"APPLE\" pattern\n",
    "                \n",
    "                # Pattern like \"_HAT\" -> should suggest 'C' or 'T'\n",
    "                if blank_pos == 0 and next_letter == 'h':\n",
    "                    # Common patterns: \"CH\", \"TH\", \"WH\"\n",
    "                    if letter in ['c', 't', 'w']:\n",
    "                        bigram = letter + 'h'\n",
    "                        if self.bigram_counts.get(bigram, 0) > 50:\n",
    "                            pattern_boost *= 2.5\n",
    "                \n",
    "                # Combine all strategies with better weighting\n",
    "                # Forward context: 40%\n",
    "                # Backward context: 30% (NEW - helps with patterns like \"_PPLE\")\n",
    "                # Position: 20%\n",
    "                # Pattern boost: multiplier\n",
    "                combined_prob = (0.4 * forward_prob + \n",
    "                               0.3 * backward_prob + \n",
    "                               0.3 * pos_prob) * pattern_boost\n",
    "                \n",
    "                position_probs.append(combined_prob)\n",
    "            \n",
    "            # Use MAX instead of MEAN (if letter fits ANY blank well, it's promising)\n",
    "            # But also consider all positions\n",
    "            letter_probs[letter] = max(position_probs) * 0.7 + np.mean(position_probs) * 0.3\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(letter_probs.values())\n",
    "        if total > 0:\n",
    "            letter_probs = {k: v/total for k, v in letter_probs.items()}\n",
    "        \n",
    "        return letter_probs\n",
    "    \n",
    "    def calculate_log_likelihood(self, word):\n",
    "        \"\"\"Calculate log-likelihood of a word: log P(word)\"\"\"\n",
    "        if len(word) == 0:\n",
    "            return float('-inf')\n",
    "        \n",
    "        log_prob = 0.0\n",
    "        \n",
    "        for i in range(len(word)):\n",
    "            letter = word[i]\n",
    "            \n",
    "            if i == 0:\n",
    "                # Start probability\n",
    "                log_prob += np.log(max(self.start_probs.get(letter, self.smoothing / self.vocab_size), 1e-10))\n",
    "            else:\n",
    "                # Transition probability\n",
    "                prev_letter = word[i-1]\n",
    "                trans_prob = self.transition_probs.get(prev_letter, {}).get(letter, self.smoothing / self.vocab_size)\n",
    "                log_prob += np.log(max(trans_prob, 1e-10))\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    def calculate_perplexity(self, corpus):\n",
    "        \"\"\"Calculate perplexity = exp(-average log likelihood)\"\"\"\n",
    "        total_log_likelihood = 0.0\n",
    "        total_letters = 0\n",
    "        \n",
    "        for word in corpus:\n",
    "            log_likelihood = self.calculate_log_likelihood(word)\n",
    "            total_log_likelihood += log_likelihood\n",
    "            total_letters += len(word)\n",
    "        \n",
    "        avg_log_likelihood = total_log_likelihood / total_letters if total_letters > 0 else 0\n",
    "        perplexity = np.exp(-avg_log_likelihood)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "print(\"1st-order HMM class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reward Function Design\n",
    "\n",
    "**Critical for RL success!** Reward design heavily impacts agent behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Function Configuration\n",
    "# These parameters are CRITICAL for RL training success!\n",
    "\n",
    "REWARD_CONFIG = {\n",
    "    'correct': 0.5,      # +reward for correct guess (immediate feedback)\n",
    "    'wrong': -1.0,       # -penalty for wrong guess (balance exploration)\n",
    "    'repeated': -0.5,    # -penalty for repeated guess (discourage inefficiency)\n",
    "    'win': 10.0,         # +bonus for winning (encourage completion)\n",
    "    'lose': -5.0,        # -penalty for losing (less than win bonus)\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REWARD FUNCTION DESIGN\")\n",
    "print(\"=\"*70)\n",
    "print(\"Reward Parameters:\")\n",
    "print(f\"  Correct guess:     +{REWARD_CONFIG['correct']:.1f}\")\n",
    "print(f\"  Wrong guess:        {REWARD_CONFIG['wrong']:.1f}\")\n",
    "print(f\"  Repeated guess:     {REWARD_CONFIG['repeated']:.1f}\")\n",
    "print(f\"  Win game:           +{REWARD_CONFIG['win']:.1f}\")\n",
    "print(f\"  Lose game:          {REWARD_CONFIG['lose']:.1f}\")\n",
    "\n",
    "print(f\"\\nüí° Tuning Guidelines:\")\n",
    "print(f\"  - Correct guess: 0.5-2.0 (immediate positive feedback)\")\n",
    "print(f\"  - Wrong guess: -0.5 to -2.0 (balance exploration vs caution)\")\n",
    "print(f\"  - Win bonus: 10-50 (encourage completing games)\")\n",
    "print(f\"  - Lose penalty: -5 to -20 (less than win bonus)\")\n",
    "print(f\"\\nCurrent Balance:\")\n",
    "total_win_reward = REWARD_CONFIG['win'] + (REWARD_CONFIG['correct'] * 5)  # Assume 5 correct guesses\n",
    "total_lose_penalty = abs(REWARD_CONFIG['lose']) + (abs(REWARD_CONFIG['wrong']) * 6)  # 6 wrong = lose\n",
    "print(f\"  Typical win reward: ~{total_win_reward:.1f}\")\n",
    "print(f\"  Typical lose penalty: ~{total_lose_penalty:.1f}\")\n",
    "if total_win_reward > total_lose_penalty:\n",
    "    print(f\"  ‚úì Win reward > lose penalty (good - encourages playing)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Win reward <= lose penalty (may discourage playing)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Update HangmanEnv to use these rewards\n",
    "# (Already defined in HangmanEnv class, but we can verify)\n",
    "print(\"\\n‚úì HangmanEnv uses these reward values\")\n",
    "print(\"  (Defined in HangmanEnv.__init__)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 HMM Training - Theory and Implementation\n",
    "\n",
    "**Training Objective:**\n",
    "Maximize likelihood: P(word) = ‚àè P(li | li-1)\n",
    "\n",
    "**Components:**\n",
    "- **Hidden States**: Letter positions/clusters\n",
    "- **Transitions**: A[i][j] = P(letter_j | letter_i)\n",
    "- **Emissions**: B[j][k] = P(obs_k | state_j)\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Additive smoothing (Œ± ‚âà 0.01-0.1) to prevent overfitting\n",
    "- ‚úÖ 1st-order HMM: letter depends on previous letter\n",
    "- ‚úÖ Position-based fallback for different word lengths\n",
    "- ‚úÖ Bigram pattern matching for common structures\n",
    "\n",
    "**Validation:**\n",
    "- Monitor training vs validation perplexity\n",
    "- Check for overfitting (validation >> training)\n",
    "- Use held-out validation set (10%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Deep Q-Network (DQN) Agent\n",
    "\n",
    "**DQN Advantages:**\n",
    "- Uses neural network to approximate Q-values (handles large state spaces)\n",
    "- Experience replay for better sample efficiency\n",
    "- Target network for stable training\n",
    "- Can learn complex patterns better than table-based Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: DQN fully removed. Using optimized Q-learning only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HMM on training corpus (NOT validation/test)\n",
    "hmm = FirstOrderHMM(smoothing=0.01)\n",
    "hmm.train(training_corpus)\n",
    "print(\"\\nHMM training complete!\")\n",
    "\n",
    "# Evaluate HMM on training and validation sets (check for overfitting)\n",
    "print(\"\\nEvaluating HMM...\")\n",
    "train_perplexity = hmm.calculate_perplexity(training_corpus[:1000])  # Sample for speed\n",
    "val_perplexity = hmm.calculate_perplexity(validation_corpus[:100])   # Sample for speed\n",
    "\n",
    "print(f\"Training Perplexity (sample): {train_perplexity:.2f}\")\n",
    "print(f\"Validation Perplexity (sample): {val_perplexity:.2f}\")\n",
    "print(f\"Difference: {abs(train_perplexity - val_perplexity):.2f}\")\n",
    "\n",
    "# Check for overfitting: if validation perplexity >> training, model is overfitting\n",
    "if val_perplexity > train_perplexity * 1.5:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Possible overfitting detected!\")\n",
    "    print(\"   Consider: reducing model complexity, increasing smoothing, or early stopping\")\n",
    "elif abs(train_perplexity - val_perplexity) < train_perplexity * 0.1:\n",
    "    print(\"‚úÖ HMM generalization looks good!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  HMM performance: validation slightly higher than training (expected)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 HMM Hyperparameters and Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM Hyperparameters (tunable)\n",
    "HMM_SMOOTHING = 0.01  # Additive smoothing parameter (Œ±)\n",
    "# Typical range: 0.01-0.1\n",
    "# Lower = more confident, but risk overfitting\n",
    "# Higher = more conservative, but may underfit\n",
    "\n",
    "print(f\"HMM Configuration:\")\n",
    "print(f\"  Smoothing (Œ±): {HMM_SMOOTHING}\")\n",
    "print(f\"  Model order: 1st-order (letter depends on previous letter)\")\n",
    "print(f\"  Validation split: 10%\")\n",
    "print(f\"\\nSmoothing effect:\")\n",
    "print(f\"  P(letter|prev) = (count + {HMM_SMOOTHING}) / (total + {HMM_SMOOTHING} * 26)\")\n",
    "\n",
    "# Tuning guidance\n",
    "print(f\"\\nüí° Tuning Tips:\")\n",
    "print(f\"  - If validation perplexity >> training: increase smoothing (reduce overfitting)\")\n",
    "print(f\"  - If perplexity too uniform: decrease smoothing (increase capacity)\")\n",
    "print(f\"  - Good balance: training ‚âà validation perplexity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HMM on example masked words\n",
    "test_cases = [\n",
    "    \"_PPLE\",      # Should suggest 'A'\n",
    "    \"_PP_E\",      # Should suggest 'A'\n",
    "    \"____\",       # 4-letter word\n",
    "    \"_HAT\",       # Should suggest 'C' or 'T'\n",
    "    \"H_NG_AN\",    # Should suggest 'A' for both blanks\n",
    "]\n",
    "\n",
    "print(\"Testing HMM predictions:\")\n",
    "for masked in test_cases:\n",
    "    probs = hmm.get_probabilities_for_mask(masked)\n",
    "    top_5 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"\\nMasked word: {masked}\")\n",
    "    print(\"Top 5 letter predictions:\")\n",
    "    for letter, prob in top_5:\n",
    "        print(f\"  {letter}: {prob:.4f}\")\n",
    "\n",
    "# Visualize transition matrix heatmap\n",
    "print(\"\\n\\nVisualizing transition matrix (most common transitions)...\")\n",
    "letters = list('abcdefghijklmnopqrstuvwxyz')\n",
    "transition_matrix = np.zeros((26, 26))\n",
    "\n",
    "for i, prev in enumerate(letters):\n",
    "    for j, curr in enumerate(letters):\n",
    "        transition_matrix[i, j] = hmm.get_letter_probability_given_prev(prev, curr)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(transition_matrix, xticklabels=letters, yticklabels=letters,\n",
    "            cmap='YlOrRd', cbar_kws={'label': 'Transition Probability'},\n",
    "            fmt='.3f', annot=False)\n",
    "plt.xlabel('Next Letter')\n",
    "plt.ylabel('Previous Letter')\n",
    "plt.title('HMM Transition Matrix: P(Next Letter | Previous Letter)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Diagonal patterns indicate common letter sequences (e.g., double letters)\")\n",
    "print(\"High values show common letter bigrams in the corpus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-Learning Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent for Hangman\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        # Learning parameters\n",
    "        self.learning_rate = learning_rate        # Œ±\n",
    "        self.discount_factor = discount_factor    # Œ≥\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon                    # Initial exploration\n",
    "        self.epsilon_decay = epsilon_decay       # Decay per episode\n",
    "        self.epsilon_min = epsilon_min           # Minimum exploration\n",
    "        \n",
    "        # Hybrid weights for combining Q and HMM\n",
    "        self.hmm_weight = hmm_weight\n",
    "        self.q_weight = q_weight\n",
    "\n",
    "        # Q-table: state ‚Üí action ‚Üí Q-value\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_stats = {\n",
    "            'episodes': [],\n",
    "            'rewards': [],\n",
    "            'wins': [],\n",
    "            'losses': []\n",
    "        }\n",
    "    \n",
    "    def state_to_key(self, state, hmm_probs):\n",
    "        \"\"\"Convert state to string key for Q-table\"\"\"\n",
    "        masked = state['masked_word']\n",
    "        guessed = ''.join(sorted(state['guessed_letters']))\n",
    "        lives = state['lives_left']\n",
    "        word_len = state['word_length']\n",
    "        \n",
    "        # Include word length and lives for better state representation\n",
    "        return f\"{masked}:{word_len}:{guessed}:{lives}\"\n",
    "    \n",
    "    def get_available_actions(self, state):\n",
    "        \"\"\"Get list of available letters to guess\"\"\"\n",
    "        all_letters = set('abcdefghijklmnopqrstuvwxyz')\n",
    "        return sorted(all_letters - state['guessed_letters'])\n",
    "    \n",
    "    def select_action(self, state, hmm_probs):\n",
    "        \"\"\"Select action using Œµ-greedy policy\"\"\"\n",
    "        available_actions = self.get_available_actions(state)\n",
    "        \n",
    "        if not available_actions:\n",
    "            return None\n",
    "        \n",
    "        # Low-lives safeguard: switch to HMM-greedy to minimize wrong guesses\n",
    "        if state.get('lives_left', 6) <= 2:\n",
    "            available_actions = self.get_available_actions(state)\n",
    "            if available_actions:\n",
    "                ordered = sorted([(a, hmm_probs.get(a,0.0)) for a in available_actions], key=lambda x: x[1], reverse=True)\n",
    "                return ordered[0][0]\n",
    "\n",
    "        # Exploration: use HMM probabilities (smart exploration)\n",
    "        if random.random() < self.epsilon:\n",
    "            # Information-gain driven exploration (with HMM guidance)\n",
    "            ig_map = info_gain_for_state(state)\n",
    "            cand = []\n",
    "            for a in available_actions:\n",
    "                cand.append((a, 0.6*ig_map.get(a,0.0) + 0.4*hmm_probs.get(a,0.0)))\n",
    "            cand.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_k = [a for a,_ in cand[:8]]\n",
    "            return random.choice(top_k if top_k else available_actions)\n",
    "        \n",
    "        # Exploitation: choose best action based on Q-values + HMM\n",
    "        state_key = self.state_to_key(state, hmm_probs)\n",
    "        \n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in available_actions:\n",
    "            q_value = self.Q[state_key][action]\n",
    "            # Combine Q-value with HMM probability (weight HMM)\n",
    "            hmm_weight = hmm_probs.get(action, 0) * 2\n",
    "            combined = self.q_weight * q_value + self.hmm_weight * hmm_probs.get(action, 0)\n",
    "            \n",
    "            if combined > best_value:\n",
    "                best_value = combined\n",
    "                best_action = action\n",
    "        \n",
    "        if best_action is None:\n",
    "            best_action = random.choice(available_actions)\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, hmm_probs, done):\n",
    "        \"\"\"Update Q-value using Q-learning - IMPROVED with HMM initialization\"\"\"\n",
    "        state_key = self.state_to_key(state, hmm_probs)\n",
    "        next_state_key = self.state_to_key(next_state, hmm_probs)\n",
    "        \n",
    "        current_q = self.Q[state_key][action]\n",
    "        \n",
    "        # IMPROVEMENT: Initialize Q-value using HMM if never seen\n",
    "        if current_q == 0 and action in hmm_probs:\n",
    "            hmm_prob = hmm_probs.get(action, 0)\n",
    "            current_q = hmm_prob * 2  # Optimistic initialization\n",
    "        \n",
    "        if done:\n",
    "            max_next_q = 0\n",
    "        else:\n",
    "            available_actions = self.get_available_actions(next_state)\n",
    "            if available_actions:\n",
    "                max_next_q = max([self.Q[next_state_key][a] for a in available_actions], default=0)\n",
    "                # Initialize next state using HMM if no Q-values\n",
    "                if max_next_q == 0:\n",
    "                    next_hmm_values = [hmm_probs.get(a, 0) * 2 for a in available_actions]\n",
    "                    max_next_q = max(next_hmm_values) if next_hmm_values else 0\n",
    "            else:\n",
    "                max_next_q = 0\n",
    "        \n",
    "        # Q-learning update with adaptive learning rate\n",
    "        adaptive_lr = self.learning_rate * (2.0 if current_q == 0 else 1.0)\n",
    "        new_q = current_q + adaptive_lr * (reward + self.discount_factor * max_next_q - current_q)\n",
    "        self.Q[state_key][action] = new_q\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save agent\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"Agent saved to {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\"Load agent\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "print(\"Q-Learning Agent class defined!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 STAGE 3: Hybrid HMM + RL Training\n",
    "\n",
    "**Integration Strategy:**\n",
    "- Combine HMM probabilities with RL state representation\n",
    "- Use HMM to guide exploration (smart exploration)\n",
    "- RL learns optimal policy given HMM information\n",
    "- Add noise to HMM occasionally to prevent over-reliance\n",
    "\n",
    "**Overfitting Prevention:**\n",
    "- ‚úÖ Periodic word shuffling (prevent memorization)\n",
    "- ‚úÖ Noise injection in HMM probabilities (10% of time)\n",
    "- ‚úÖ Validation monitoring during training\n",
    "- ‚úÖ Train/test separation (no data leakage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "agent = QLearningAgent(**RL_CONFIG)\n",
    "\n",
    "print(\"Agent initialized!\")\n",
    "print(f\"Initial epsilon: {agent.epsilon}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop - Online Learning\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ **HMM probabilities recalculated after EVERY guess** - As masked_word changes, probabilities update\n",
    "- ‚úÖ **Q-values updated IMMEDIATELY after each action** - Online learning (not batch learning)\n",
    "- ‚úÖ **Step-by-step learning** - Agent learns from each guess, not just at episode end\n",
    "\n",
    "**Learning Flow:**\n",
    "1. Get HMM probabilities for current state (masked_word, guessed_letters)\n",
    "2. Agent selects action (letter to guess)\n",
    "3. Execute action ‚Üí get reward, new state\n",
    "4. Recalculate HMM probabilities for NEW state (probabilities change after reveal!)\n",
    "5. Update Q-values IMMEDIATELY using (state, action, reward, next_state)\n",
    "6. Move to next iteration\n",
    "\n",
    "This is **online/temporal-difference learning** - learns continuously, not in batches!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "NUM_EPISODES = 5000  # Further increased for better performance (was 3000)\n",
    "TRAINING_SUBSET = 10000  # More diverse training words (was 8000)\n",
    "\n",
    "# IMPORTANT: Use only TRAINING corpus (not validation or test!)\n",
    "# This prevents data leakage\n",
    "\n",
    "# Strategy 1: Sample proportionally from each bucket (maintains length distribution)\n",
    "def sample_from_buckets(buckets, total_samples):\n",
    "    \"\"\"Sample words proportionally from each bucket\"\"\"\n",
    "    # Calculate total words\n",
    "    total_words = sum(len(words) for words in buckets.values())\n",
    "    \n",
    "    # Sample proportionally from each bucket\n",
    "    sampled_words = []\n",
    "    for length, words in buckets.items():\n",
    "        proportion = len(words) / total_words\n",
    "        samples_from_bucket = max(1, int(total_samples * proportion))\n",
    "        sampled_words.extend(random.sample(words, min(samples_from_bucket, len(words))))\n",
    "    \n",
    "    return sampled_words\n",
    "\n",
    "# Strategy 2: Sample uniformly across lengths (equal representation per length)\n",
    "def sample_uniform_from_buckets(buckets, samples_per_length):\n",
    "    \"\"\"Sample equal number of words from each length bucket\"\"\"\n",
    "    sampled_words = []\n",
    "    for length, words in buckets.items():\n",
    "        if len(words) > 0:\n",
    "            samples = min(samples_per_length, len(words))\n",
    "            sampled_words.extend(random.sample(words, samples))\n",
    "    return sampled_words\n",
    "\n",
    "# Choose sampling strategy\n",
    "# Strategy 1: Proportional (maintains natural distribution)\n",
    "rl_training_words = sample_from_buckets(training_buckets, min(TRAINING_SUBSET, len(training_corpus)))\n",
    "if not rl_training_words or len(rl_training_words) == 0:\n",
    "    # Fallback: use training_corpus directly (shuffled)\n",
    "    rl_training_words = training_corpus.copy()\n",
    "    random.shuffle(rl_training_words)\n",
    "# Final guard: if still empty, fallback to test_words subset\n",
    "if not rl_training_words or len(rl_training_words) == 0:\n",
    "    rl_training_words = test_words[:1000] if len(test_words) > 0 else []\n",
    "\n",
    "\n",
    "# Strategy 2: Uniform (uncomment to use instead)\n",
    "# SAMPLES_PER_LENGTH = TRAINING_SUBSET // len(training_buckets)  # Distribute evenly\n",
    "# rl_training_words = sample_uniform_from_buckets(training_buckets, SAMPLES_PER_LENGTH)\n",
    "\n",
    "print(f\"RL Training Configuration:\")\n",
    "print(f\"  Training words: {len(rl_training_words)}\")\n",
    "print(f\"  Number of episodes: {NUM_EPISODES}\")\n",
    "print(f\"  Validation set: {len(validation_corpus)} words (for monitoring)\")\n",
    "print(f\"  Test set: {len(test_words)} words (FINAL evaluation only)\")\n",
    "\n",
    "# Show distribution of sampled training words\n",
    "training_sample_buckets = bucket_words_by_length(rl_training_words)\n",
    "print(f\"\\nSampled training words distribution:\")\n",
    "for length in sorted(training_sample_buckets.keys()):\n",
    "    print(f\"  Length {length:2d}: {len(training_sample_buckets[length]):4d} words\")\n",
    "\n",
    "# Prepare validation subset for RL evaluation during training\n",
    "val_subset = sample_from_buckets(validation_buckets, min(500, len(validation_corpus)))\n",
    "if not val_subset:\n",
    "    val_subset = validation_corpus[:500]\n",
    "print(f\"\\n  Validation subset for RL monitoring: {len(val_subset)} words\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation monitoring (to detect overfitting)\n",
    "episode_rewards = []\n",
    "episode_wins = []\n",
    "episode_losses = []\n",
    "episode_wrong_guesses = []\n",
    "episode_step_counts = []  # Track steps per episode\n",
    "\n",
    "# Validation metrics (to track overfitting)\n",
    "validation_win_rates = []\n",
    "validation_episodes = []\n",
    "\n",
    "# Per-step learning statistics (optional - for analysis)\n",
    "step_rewards = []  # Reward at each step (across all episodes)\n",
    "step_q_updates = 0  # Count total Q-value updates\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RL TRAINING - ONLINE LEARNING (Updates after EVERY guess)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Key features:\")\n",
    "print(\"  ‚úì HMM probabilities recalculated after each guess\")\n",
    "print(\"  ‚úì Q-values updated immediately after each action\")\n",
    "print(\"  ‚úì Learning happens step-by-step, not episode-by-episode\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nStarting training on {NUM_EPISODES} episodes...\")\n",
    "print(f\"Progress every {NUM_EPISODES // 10} episodes:\")\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Initialize bucket structure\n",
    "    if episode == 0:\n",
    "        training_sample_buckets = bucket_words_by_length(rl_training_words)\n",
    "    # IMPORTANT: Shuffle training words periodically to prevent memorization\n",
    "    if episode % 100 == 0:\n",
    "        random.shuffle(rl_training_words)\n",
    "        # Re-bucket if needed (optional)\n",
    "        training_sample_buckets = bucket_words_by_length(rl_training_words)\n",
    "    \n",
    "    # Sample a random word from TRAINING set only\n",
    "    # Strategy 1: Sample uniformly across all lengths (better exploration)\n",
    "    # Choose a random length, then a random word from that length\n",
    "    available_lengths = [l for l in training_sample_buckets.keys() if len(training_sample_buckets[l]) > 0]\n",
    "    if len(available_lengths) == 0:\n",
    "        available_lengths = list(training_buckets.keys())\n",
    "    \n",
    "    # Option A: Uniform across lengths (uncomment to use)\n",
    "    # chosen_length = random.choice(available_lengths)\n",
    "    # word = random.choice(training_sample_buckets[chosen_length])\n",
    "    \n",
    "    # Option B: Proportional to bucket size (default - maintains natural distribution)\n",
    "    # Sample a random word from TRAINING set only\n",
    "    word = random.choice(rl_training_words) if rl_training_words else random.choice(training_corpus)\n",
    "    env = HangmanEnv(word, max_lives=6, max_guesses=25)\n",
    "    state = env.get_state()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_wrong = 0\n",
    "    done = False\n",
    "    \n",
    "    # Add noise to HMM probabilities occasionally (prevent over-reliance)\n",
    "    use_noisy_hmm = random.random() < 0.1  # 10% of time, add noise\n",
    "    \n",
    "    step_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        \n",
    "        # ============================================\n",
    "        # STEP 1: Get HMM probabilities for CURRENT state\n",
    "        # ============================================\n",
    "        # IMPORTANT: Recalculate probabilities after each guess!\n",
    "        # The masked_word and guessed_letters change after each guess\n",
    "        hmm_probs = get_letter_probs(hmm, state)\n",
    "        \n",
    "        # Optionally add noise to prevent overfitting to HMM quirks\n",
    "        if use_noisy_hmm:\n",
    "            # Add small Gaussian noise\n",
    "            noisy_probs = {}\n",
    "            for letter, prob in hmm_probs.items():\n",
    "                noise = np.random.normal(0, prob * 0.1)  # 10% noise\n",
    "                noisy_probs[letter] = max(0, prob + noise)\n",
    "            # Renormalize\n",
    "            total = sum(noisy_probs.values())\n",
    "            if total > 0:\n",
    "                hmm_probs = {k: v/total for k, v in noisy_probs.items()}\n",
    "        \n",
    "        # ============================================\n",
    "        # STEP 2: Agent selects action based on current state + HMM probs\n",
    "        # ============================================\n",
    "        action = agent.select_action(state, hmm_probs)\n",
    "        \n",
    "        if action is None:\n",
    "            break\n",
    "        \n",
    "        # ============================================\n",
    "        # STEP 3: Execute action in environment\n",
    "        # ============================================\n",
    "        reward, next_state, done, info = env.guess_letter(action)\n",
    "        \n",
    "        if info['status'] == 'wrong':\n",
    "            episode_wrong += 1\n",
    "        \n",
    "        # ============================================\n",
    "        # STEP 4: Recalculate HMM probabilities for NEW state\n",
    "        # ============================================\n",
    "        # CRITICAL: Get updated probabilities after the guess!\n",
    "        # The masked_word has changed (letters revealed), so probabilities change\n",
    "        next_hmm_probs = get_letter_probs(hmm, next_state)\n",
    "        \n",
    "        # ============================================\n",
    "        # STEP 5: UPDATE Q-VALUES IMMEDIATELY (after each guess)\n",
    "        # ============================================\n",
    "        # Learn from this experience RIGHT AWAY - don't wait for episode end!\n",
    "        # This is online learning: update after every single action\n",
    "        agent.update(\n",
    "            state=state,           # Current state\n",
    "            action=action,          # Action taken\n",
    "            reward=reward,          # Reward received\n",
    "            next_state=next_state,  # New state after guess\n",
    "            hmm_probs=next_hmm_probs,  # Updated HMM probs for new state\n",
    "            done=done               # Whether episode ended\n",
    "        )\n",
    "        \n",
    "        # ============================================\n",
    "        # STEP 6: Move to next state for next iteration\n",
    "        # ============================================\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Optional: Track per-step learning (for debugging/analysis)\n",
    "        # You can log Q-value changes here if needed\n",
    "    \n",
    "    # Track statistics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    if info['status'] == 'won':\n",
    "        episode_wins.append(1)\n",
    "        episode_losses.append(0)\n",
    "    elif info['status'] == 'lost':\n",
    "        episode_wins.append(0)\n",
    "        episode_losses.append(1)\n",
    "    else:\n",
    "        episode_wins.append(0)\n",
    "        episode_losses.append(0)\n",
    "    \n",
    "    episode_wrong_guesses.append(episode_wrong)\n",
    "    episode_step_counts.append(step_count)  # Track steps in this episode\n",
    "    \n",
    "    # Log learning progress\n",
    "    step_q_updates += step_count  # Each step = one Q-value update\n",
    "    \n",
    "    # Periodic validation check (to detect overfitting)\n",
    "    if (episode + 1) % (NUM_EPISODES // 5) == 0:\n",
    "        # Evaluate on validation set (unseen words)\n",
    "        val_wins = 0\n",
    "        for val_word in val_subset[:100]:  # Sample 100 validation words\n",
    "            env_val = HangmanEnv(val_word, max_lives=6, max_guesses=25)\n",
    "            state_val = env_val.get_state()\n",
    "            done_val = False\n",
    "            \n",
    "            # Use greedy policy (no exploration) for evaluation\n",
    "            old_epsilon = agent.epsilon\n",
    "            agent.epsilon = 0\n",
    "            \n",
    "            while not done_val:\n",
    "                hmm_probs_val = hmm.get_probabilities_for_mask(state_val['masked_word'], state_val['guessed_letters'])\n",
    "                action_val = agent.select_action(state_val, hmm_probs_val)\n",
    "                if action_val is None:\n",
    "                    break\n",
    "                _, state_val, done_val, info_val = env_val.guess_letter(action_val)\n",
    "                if done_val and info_val['status'] == 'won':\n",
    "                    val_wins += 1\n",
    "            \n",
    "            agent.epsilon = old_epsilon\n",
    "        \n",
    "        val_win_rate = val_wins / min(100, len(val_subset))\n",
    "        validation_win_rates.append(val_win_rate)\n",
    "        validation_episodes.append(episode + 1)\n",
    "    \n",
    "    # Progress update\n",
    "    if (episode + 1) % (NUM_EPISODES // 10) == 0:\n",
    "        recent_win_rate = np.mean(episode_wins[-100:]) if len(episode_wins) >= 100 else np.mean(episode_wins)\n",
    "        recent_avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "        print(f\"Episode {episode + 1}/{NUM_EPISODES} | \"\n",
    "              f\"Train Win Rate: {recent_win_rate:.2%} | \"\n",
    "              f\"Avg Reward: {recent_avg_reward:.2f} | \"\n",
    "              f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - ONLINE LEARNING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total episodes: {NUM_EPISODES}\")\n",
    "print(f\"Total Q-value updates (learning steps): {step_q_updates}\")\n",
    "print(f\"Average steps per episode: {np.mean(episode_step_counts):.2f}\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.3f}\")\n",
    "print(f\"Total states in Q-table: {len(agent.Q)}\")\n",
    "print(f\"Average Q-table size growth: {len(agent.Q) / NUM_EPISODES:.2f} states per episode\")\n",
    "print(f\"\\n‚úì Learning happened after EVERY guess (online learning)\")\n",
    "print(f\"‚úì HMM probabilities recalculated after each guess\")\n",
    "print(f\"‚úì Total learning experiences: {step_q_updates}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for overfitting: if training win rate >> validation win rate\n",
    "if len(validation_win_rates) > 0:\n",
    "    final_train_win_rate = np.mean(episode_wins[-100:]) if len(episode_wins) >= 100 else np.mean(episode_wins)\n",
    "    final_val_win_rate = validation_win_rates[-1]\n",
    "    print(f\"\\nOverfitting Check:\")\n",
    "    print(f\"  Final Training Win Rate: {final_train_win_rate:.2%}\")\n",
    "    print(f\"  Final Validation Win Rate: {final_val_win_rate:.2%}\")\n",
    "    if final_train_win_rate > final_val_win_rate * 1.5:\n",
    "        print(\"  ‚ö†Ô∏è  WARNING: Possible overfitting detected!\")\n",
    "    elif abs(final_train_win_rate - final_val_win_rate) < final_train_win_rate * 0.15:\n",
    "        print(\"  ‚úÖ Good generalization!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Reward over episodes\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "# Moving average\n",
    "window = 50\n",
    "if len(episode_rewards) >= window:\n",
    "    moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "    axes[0, 0].plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', label=f'{window}-Episode Average')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].set_title('Reward Over Episodes')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Win rate over episodes\n",
    "win_rates = []\n",
    "for i in range(1, len(episode_wins) + 1):\n",
    "    win_rates.append(np.mean(episode_wins[:i]))\n",
    "axes[0, 1].plot(win_rates, label='Cumulative Win Rate')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Win Rate')\n",
    "axes[0, 1].set_title('Win Rate Over Episodes')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Wrong guesses over episodes\n",
    "axes[1, 0].plot(episode_wrong_guesses, alpha=0.3, label='Wrong Guesses')\n",
    "if len(episode_wrong_guesses) >= window:\n",
    "    moving_avg_wrong = np.convolve(episode_wrong_guesses, np.ones(window)/window, mode='valid')\n",
    "    axes[1, 0].plot(range(window-1, len(episode_wrong_guesses)), moving_avg_wrong, 'r-', label=f'{window}-Episode Average')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Wrong Guesses')\n",
    "axes[1, 0].set_title('Wrong Guesses Over Episodes')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Epsilon decay\n",
    "epsilon_values = []\n",
    "epsilon = 1.0\n",
    "for _ in range(NUM_EPISODES):\n",
    "    epsilon_values.append(epsilon)\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= 0.995\n",
    "axes[1, 1].plot(epsilon_values, label='Epsilon')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].set_title('Exploration (Epsilon) Over Episodes')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Total Episodes: {NUM_EPISODES}\")\n",
    "print(f\"Overall Win Rate: {np.mean(episode_wins):.2%}\")\n",
    "print(f\"Average Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Average Wrong Guesses: {np.mean(episode_wrong_guesses):.2f}\")\n",
    "print(f\"Recent 100 Episodes Win Rate: {np.mean(episode_wins[-100:]):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Health Metrics Summary\n",
    "\n",
    "**Quantitative Indicators of Healthy Training:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Training Health Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE TRAINING HEALTH REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. HMM Training Health:\")\n",
    "if 'train_perplexity' in locals() and 'val_perplexity' in locals():\n",
    "    if abs(train_perplexity - val_perplexity) < train_perplexity * 0.1:\n",
    "        print(f\"   ‚úÖ HMM: Training ‚âà Validation perplexity (Good generalization)\")\n",
    "    elif val_perplexity > train_perplexity * 1.5:\n",
    "        print(f\"   ‚ö†Ô∏è  HMM: Overfitting detected (val >> train)\")\n",
    "    else:\n",
    "        print(f\"   ‚úì HMM: Acceptable generalization gap\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  HMM: Perplexity not calculated - run HMM validation cell\")\n",
    "\n",
    "print(\"\\n2. RL Training Health:\")\n",
    "if len(episode_rewards) > 0:\n",
    "    # Check reward curve trend\n",
    "    early_reward = np.mean(episode_rewards[:100]) if len(episode_rewards) >= 100 else np.mean(episode_rewards[:len(episode_rewards)//4])\n",
    "    late_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards[-len(episode_rewards)//4:])\n",
    "    \n",
    "    if late_reward > early_reward * 1.2:\n",
    "        print(f\"   ‚úÖ RL: Reward curve rising (learning effectively)\")\n",
    "    elif late_reward > early_reward:\n",
    "        print(f\"   ‚úì RL: Reward curve improving (slow but learning)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  RL: Reward not improving (may need tuning)\")\n",
    "    \n",
    "    # Check win rate trend\n",
    "    early_win = np.mean(episode_wins[:100]) if len(episode_wins) >= 100 else np.mean(episode_wins[:len(episode_wins)//4])\n",
    "    late_win = np.mean(episode_wins[-100:]) if len(episode_wins) >= 100 else np.mean(episode_wins[-len(episode_wins)//4:])\n",
    "    \n",
    "    if late_win > early_win * 1.5:\n",
    "        print(f\"   ‚úÖ RL: Win rate improving significantly ({early_win:.1%} ‚Üí {late_win:.1%})\")\n",
    "    elif late_win > early_win:\n",
    "        print(f\"   ‚úì RL: Win rate improving ({early_win:.1%} ‚Üí {late_win:.1%})\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  RL: Win rate stagnant ({late_win:.1%})\")\n",
    "\n",
    "print(\"\\n3. Validation vs Training:\")\n",
    "if len(validation_win_rates) > 0:\n",
    "    final_train = np.mean(episode_wins[-100:]) if len(episode_wins) >= 100 else np.mean(episode_wins)\n",
    "    final_val = validation_win_rates[-1]\n",
    "    gap = abs(final_train - final_val) / final_train if final_train > 0 else 0\n",
    "    \n",
    "    if gap < 0.15:\n",
    "        print(f\"   ‚úÖ Generalization: Training ‚âà Validation (gap < 15%)\")\n",
    "    elif gap < 0.30:\n",
    "        print(f\"   ‚úì Generalization: Moderate gap ({gap*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Generalization: Large gap ({gap*100:.1f}%) - possible overfitting\")\n",
    "\n",
    "print(\"\\n4. Wrong Guesses Trend:\")\n",
    "if len(episode_wrong_guesses) > 100:\n",
    "    early_wrong = np.mean(episode_wrong_guesses[:100])\n",
    "    late_wrong = np.mean(episode_wrong_guesses[-100:])\n",
    "    if late_wrong < early_wrong * 0.8:\n",
    "        print(f\"   ‚úÖ Wrong guesses decreasing ({early_wrong:.2f} ‚Üí {late_wrong:.2f})\")\n",
    "    elif late_wrong < early_wrong:\n",
    "        print(f\"   ‚úì Wrong guesses improving ({early_wrong:.2f} ‚Üí {late_wrong:.2f})\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Wrong guesses not improving ({late_wrong:.2f})\")\n",
    "\n",
    "print(\"\\n5. Repeated Guesses:\")\n",
    "if 'total_repeated' in locals():\n",
    "    if total_repeated == 0:\n",
    "        print(f\"   ‚úÖ Zero repeated guesses (perfect!)\")\n",
    "    elif total_repeated < len(test_words) * 0.05:\n",
    "        print(f\"   ‚úì Low repeated guesses ({total_repeated}, < 5% of games)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Too many repeated guesses ({total_repeated})\")\n",
    "\n",
    "print(\"\\n6. Overall Assessment:\")\n",
    "all_good = True\n",
    "if 'train_perplexity' in locals() and val_perplexity > train_perplexity * 1.5:\n",
    "    all_good = False\n",
    "if len(validation_win_rates) > 0:\n",
    "    gap = abs(np.mean(episode_wins[-100:]) - validation_win_rates[-1])\n",
    "    if gap > np.mean(episode_wins[-100:]) * 0.30:\n",
    "        all_good = False\n",
    "\n",
    "if all_good and success_rate >= 0.6:\n",
    "    print(f\"   ‚úÖ Overall: Healthy training, good performance!\")\n",
    "elif success_rate >= 0.4:\n",
    "    print(f\"   ‚úì Overall: Acceptable performance, may benefit from more training\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Overall: Needs improvement - consider tuning hyperparameters\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_agent(agent, hmm, test_words, max_lives=6):\n",
    "    \"\"\"Evaluate agent on test set\"\"\"\n",
    "    results = {\n",
    "        'wins': 0,\n",
    "        'losses': 0,\n",
    "        'total_wrong_guesses': 0,\n",
    "        'total_repeated_guesses': 0,\n",
    "        'total_guesses': 0,\n",
    "        'game_results': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_words)} test words...\")\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        env = HangmanEnv(word, max_lives=max_lives, max_guesses=25)\n",
    "        state = env.get_state()\n",
    "        done = False\n",
    "        wrong_guesses = 0\n",
    "        repeated_guesses = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get HMM probabilities\n",
    "            hmm_probs = hmm.get_probabilities_for_mask(state['masked_word'], state['guessed_letters'])\n",
    "            \n",
    "            # Agent selects action (use greedy policy - no exploration)\n",
    "            # Temporarily set epsilon to 0 for evaluation\n",
    "            # old_epsilon = agent.epsilon\n",
    "            # agent.epsilon = 0\n",
    "            action = agent.select_action(state, hmm_probs)\n",
    "            # agent.epsilon = old_epsilon\n",
    "            \n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            # Execute action\n",
    "            reward, next_state, done, info = env.guess_letter(action)\n",
    "            \n",
    "            if info['status'] == 'wrong':\n",
    "                wrong_guesses += 1\n",
    "            elif info['status'] == 'repeated':\n",
    "                repeated_guesses += 1\n",
    "            \n",
    "            state = next_state\n",
    "            results['total_guesses'] += 1\n",
    "        \n",
    "        # Record game result\n",
    "        if info['status'] == 'won':\n",
    "            results['wins'] += 1\n",
    "        else:\n",
    "            results['losses'] += 1\n",
    "        \n",
    "        results['total_wrong_guesses'] += wrong_guesses\n",
    "        results['total_repeated_guesses'] += repeated_guesses\n",
    "        \n",
    "        results['game_results'].append({\n",
    "            'word': word,\n",
    "            'won': info['status'] == 'won',\n",
    "            'wrong_guesses': wrong_guesses,\n",
    "            'repeated_guesses': repeated_guesses\n",
    "        })\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(test_words)} words...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "# Ensure test_words is not empty; try fallbacks if needed\n",
    "if (\"test_words\" not in globals()) or (len(test_words) == 0):\n",
    "    print(\"Warning: test_words empty. Trying to recover...\")\n",
    "    try:\n",
    "        # Attempt to use raw test and reapply minimal filtering\n",
    "        if \"test_words_raw\" in globals() and len(test_words_raw) > 0:\n",
    "            test_words = [w.strip().lower() for w in test_words_raw if w and w.strip().isalpha()]\n",
    "        elif \"validation_corpus\" in globals() and len(validation_corpus) > 0:\n",
    "            test_words = validation_corpus[:min(500, len(validation_corpus))]\n",
    "        elif \"corpus\" in globals() and len(corpus) > 0:\n",
    "            test_words = corpus[-min(500, len(corpus)):]\n",
    "        else:\n",
    "            test_words = []\n",
    "    except Exception as _e:\n",
    "        print(\"Recovery failed:\", _e)\n",
    "        test_words = []\n",
    "\n",
    "print(f\"Evaluating on {len(test_words)} test words...\")\n",
    "evaluation_results = evaluate_agent(agent, hmm, test_words, max_lives=6)\n",
    "\n",
    "# Calculate final score\n",
    "len_test = max(1, len(test_words))\n",
    "success_rate = evaluation_results['wins'] / len_test\n",
    "total_wrong = evaluation_results['total_wrong_guesses']\n",
    "total_repeated = evaluation_results['total_repeated_guesses']\n",
    "\n",
    "final_score = (success_rate * 2000) - (total_wrong * 5) - (total_repeated * 2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Test Words: {len(test_words)}\")\n",
    "print(f\"Wins: {evaluation_results['wins']}\")\n",
    "print(f\"Losses: {evaluation_results['losses']}\")\n",
    "print(f\"Success Rate: {success_rate:.2%}\")\n",
    "print(f\"\\nTotal Wrong Guesses: {total_wrong}\")\n",
    "print(f\"Average Wrong Guesses per Game: {total_wrong / max(1, len(test_words)):.2f}\")\n",
    "print(f\"\\nTotal Repeated Guesses: {total_repeated}\")\n",
    "print(f\"Average Repeated Guesses per Game: {total_repeated / max(1, len(test_words)):.2f}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL SCORE: {final_score:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create models directory if it doesn't exist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mos\u001b[49m.makedirs(\u001b[33m'\u001b[39m\u001b[33m../models\u001b[39m\u001b[33m'\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save HMM model\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../models/hmm_model.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save HMM model\n",
    "with open('../models/hmm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(hmm, f)\n",
    "print(\"HMM model saved to ../models/hmm_model.pkl\")\n",
    "\n",
    "# Save RL agent\n",
    "agent.save('../models/rl_agent.pkl')\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Analysis (Optional)\n",
    "\n",
    "### Analyze performance by word length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyze performance by word length\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m length_stats = \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[33m'\u001b[39m\u001b[33mwins\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwrong_guesses\u001b[39m\u001b[33m'\u001b[39m: []})\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m evaluation_results[\u001b[33m'\u001b[39m\u001b[33mgame_results\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      5\u001b[39m     word = result[\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "# Analyze performance by word length\n",
    "length_stats = defaultdict(lambda: {'wins': 0, 'total': 0, 'wrong_guesses': []})\n",
    "\n",
    "for result in evaluation_results['game_results']:\n",
    "    word = result['word']\n",
    "    word_len = len(word)\n",
    "    length_stats[word_len]['total'] += 1\n",
    "    if result['won']:\n",
    "        length_stats[word_len]['wins'] += 1\n",
    "    length_stats[word_len]['wrong_guesses'].append(result['wrong_guesses'])\n",
    "\n",
    "# Enhanced visualization by word length buckets\n",
    "lengths = sorted(length_stats.keys())\n",
    "win_rates_by_length = [length_stats[l]['wins'] / length_stats[l]['total'] if length_stats[l]['total'] > 0 else 0 for l in lengths]\n",
    "avg_wrong_by_length = [np.mean(length_stats[l]['wrong_guesses']) if len(length_stats[l]['wrong_guesses']) > 0 else 0 for l in lengths]\n",
    "avg_repeated_by_length = [np.mean(length_stats[l]['repeated_guesses']) if len(length_stats[l]['repeated_guesses']) > 0 else 0 for l in lengths]\n",
    "word_counts_by_length = [length_stats[l]['total'] for l in lengths]\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Win rate by length\n",
    "axes[0, 0].bar(lengths, win_rates_by_length, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Word Length')\n",
    "axes[0, 0].set_ylabel('Win Rate')\n",
    "axes[0, 0].set_title('Win Rate by Word Length Bucket')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Average wrong guesses by length\n",
    "axes[0, 1].bar(lengths, avg_wrong_by_length, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Word Length')\n",
    "axes[0, 1].set_ylabel('Average Wrong Guesses')\n",
    "axes[0, 1].set_title('Average Wrong Guesses by Word Length Bucket')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Average repeated guesses by length\n",
    "axes[1, 0].bar(lengths, avg_repeated_by_length, color='orange', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Word Length')\n",
    "axes[1, 0].set_ylabel('Average Repeated Guesses')\n",
    "axes[1, 0].set_title('Average Repeated Guesses by Word Length Bucket')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Word count distribution in test set\n",
    "axes[1, 1].bar(lengths, word_counts_by_length, color='green', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Word Length')\n",
    "axes[1, 1].set_ylabel('Number of Words')\n",
    "axes[1, 1].set_title('Test Set Distribution by Word Length')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: Correlation between word length and performance\n",
    "if len(lengths) > 1:\n",
    "    correlation_wrong = np.corrcoef(lengths, avg_wrong_by_length)[0, 1]\n",
    "    correlation_win = np.corrcoef(lengths, win_rates_by_length)[0, 1]\n",
    "    \n",
    "    print(f\"\\nüìä Correlation Analysis:\")\n",
    "    print(f\"  Word Length vs Win Rate: {correlation_win:.3f}\")\n",
    "    print(f\"  Word Length vs Wrong Guesses: {correlation_wrong:.3f}\")\n",
    "    \n",
    "    if correlation_wrong > 0.5:\n",
    "        print(f\"  üí° Longer words tend to have more wrong guesses\")\n",
    "    if correlation_win < -0.3:\n",
    "        print(f\"  üí° Longer words tend to have lower win rates\")\n",
    "\n",
    "print(\"\\nPerformance by Word Length:\")\n",
    "for length in sorted(length_stats.keys()):\n",
    "    stats = length_stats[length]\n",
    "    win_rate = stats['wins'] / stats['total']\n",
    "    avg_wrong = np.mean(stats['wrong_guesses'])\n",
    "    print(f\"Length {length:2d}: Win Rate {win_rate:.2%}, Avg Wrong {avg_wrong:.2f} ({stats['total']} words)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
